{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ΕΡΓΑΣΙΑ ΓΙΑ ΤΟ ΜΑΘΗΜΑ ΘΕΩΡΙΑ ΔΙΚΤΥΩΝ ΜΕ ΘΕΜΑ ΤΗΝ ΕΥΡΕΣΗ ΚΟΙΝΟΤΗΤΩΝ**\n",
    "<br>ΘΩΜΑΣ ΚΥΡΙΑΚΟΣ ΠΡΑΒΙΝΟΣ \n",
    "<br>ΑΕΜ : 9937"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import community.community_louvain as community_louvain\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "df = df.head(500)\n",
    "\n",
    "# Load the sentence transformer model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate sentence embeddings for the articles using their Titles \n",
    "sentences = df['TITLE'].tolist()\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Create a NetworkX graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes to the graph, one for each article\n",
    "for i, row in df.iterrows():\n",
    "    G.add_node(row['ID'], title=row['TITLE'])\n",
    "\n",
    "# Add edges to the graph, connecting nodes that are similar according the cosine similarity between their embeddings\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(i+1, len(sentences)):\n",
    "        # Calculate the similarity between the ith and jth articles\n",
    "        # using the cosine similarity between their embeddings\n",
    "        sim = cosine_similarity(embeddings[i].reshape(1, -1), embeddings[j].reshape(1, -1))[0][0]\n",
    "        \n",
    "        # If the similarity is above some threshold, add an edge between the nodes\n",
    "        if sim > 0.2:\n",
    "            G.add_edge(df.iloc[i]['ID'], df.iloc[j]['ID'], weight=sim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modularity:  0.3730438520678947\n",
      "Community  1 :  [1, 2, 21, 27, 35, 40, 45, 46, 49, 54, 59, 61, 66, 69, 82, 96, 97, 101, 108, 109, 112, 117, 125, 151, 169, 178, 179, 192, 193, 198, 205, 206, 211, 217, 220, 221, 226, 227, 232, 234, 238, 240, 247, 250, 254, 255, 256, 258, 261, 270, 272, 275, 277, 280, 287, 294, 299, 303, 313, 314, 317, 318, 322, 329, 330, 337, 345, 347, 355, 360, 361, 366, 381, 384, 397, 401, 405, 407, 408, 410, 418, 426, 430, 432, 434, 436, 437, 439, 445, 451, 453, 464, 468, 476, 477, 485, 498] , count:  97\n",
      "Community  2 :  [3, 6, 14, 18, 25, 28, 32, 47, 55, 67, 68, 70, 79, 84, 88, 98, 99, 105, 107, 111, 113, 129, 130, 133, 134, 143, 144, 147, 148, 152, 153, 154, 155, 163, 164, 170, 171, 176, 184, 188, 200, 202, 207, 210, 213, 218, 219, 236, 249, 276, 284, 288, 302, 304, 310, 312, 316, 321, 333, 340, 341, 343, 348, 352, 353, 354, 356, 365, 368, 371, 375, 382, 388, 389, 391, 400, 406, 414, 415, 416, 417, 423, 424, 429, 433, 435, 440, 443, 449, 450, 454, 459, 462, 488, 490, 496, 497, 499, 500] , count:  99\n",
      "Community  3 :  [4, 7, 8, 9, 11, 13, 15, 16, 17, 20, 22, 26, 37, 39, 43, 48, 52, 57, 63, 64, 65, 72, 75, 78, 80, 86, 87, 89, 91, 92, 94, 95, 100, 102, 104, 106, 114, 123, 124, 126, 127, 132, 136, 137, 138, 146, 156, 162, 167, 172, 181, 182, 183, 190, 191, 199, 201, 204, 208, 209, 212, 216, 222, 231, 237, 239, 241, 245, 252, 263, 264, 266, 268, 274, 282, 283, 289, 290, 292, 293, 296, 297, 300, 305, 311, 315, 323, 327, 335, 338, 339, 344, 351, 357, 358, 359, 362, 367, 370, 374, 376, 377, 379, 380, 383, 386, 398, 399, 409, 411, 412, 419, 420, 421, 425, 428, 438, 441, 446, 447, 452, 455, 456, 458, 460, 465, 469, 472, 473, 474, 478, 480, 481, 486, 494] , count:  135\n",
      "Community  4 :  [5, 19, 29, 30, 31, 36, 41, 42, 44, 51, 71, 76, 81, 90, 93, 110, 119, 120, 128, 131, 135, 145, 159, 165, 166, 173, 185, 186, 195, 214, 224, 225, 230, 235, 242, 244, 248, 251, 257, 260, 267, 269, 271, 273, 281, 285, 286, 291, 295, 301, 306, 308, 309, 319, 320, 324, 325, 326, 331, 334, 336, 346, 349, 369, 372, 378, 393, 394, 396, 402, 431, 442, 448, 457, 466, 467, 470, 479, 482, 483, 487, 489, 492, 493] , count:  84\n",
      "Community  5 :  [10, 12, 23, 24, 33, 34, 38, 50, 53, 56, 58, 60, 62, 73, 74, 77, 83, 85, 103, 115, 116, 118, 121, 122, 139, 140, 141, 142, 149, 157, 158, 160, 161, 168, 174, 175, 177, 180, 187, 189, 194, 196, 197, 203, 215, 223, 228, 229, 233, 243, 246, 253, 259, 262, 265, 278, 279, 298, 307, 328, 332, 342, 350, 363, 364, 373, 385, 387, 390, 392, 395, 403, 404, 413, 422, 427, 444, 461, 463, 471, 475, 484, 491, 495] , count:  84\n",
      "Community  6 :  [150] , count:  1\n",
      "[[1, 2, 21, 27, 35, 40, 45, 46, 49, 54, 59, 61, 66, 69, 82, 96, 97, 101, 108, 109, 112, 117, 125, 151, 169, 178, 179, 192, 193, 198, 205, 206, 211, 217, 220, 221, 226, 227, 232, 234, 238, 240, 247, 250, 254, 255, 256, 258, 261, 270, 272, 275, 277, 280, 287, 294, 299, 303, 313, 314, 317, 318, 322, 329, 330, 337, 345, 347, 355, 360, 361, 366, 381, 384, 397, 401, 405, 407, 408, 410, 418, 426, 430, 432, 434, 436, 437, 439, 445, 451, 453, 464, 468, 476, 477, 485, 498], [3, 6, 14, 18, 25, 28, 32, 47, 55, 67, 68, 70, 79, 84, 88, 98, 99, 105, 107, 111, 113, 129, 130, 133, 134, 143, 144, 147, 148, 152, 153, 154, 155, 163, 164, 170, 171, 176, 184, 188, 200, 202, 207, 210, 213, 218, 219, 236, 249, 276, 284, 288, 302, 304, 310, 312, 316, 321, 333, 340, 341, 343, 348, 352, 353, 354, 356, 365, 368, 371, 375, 382, 388, 389, 391, 400, 406, 414, 415, 416, 417, 423, 424, 429, 433, 435, 440, 443, 449, 450, 454, 459, 462, 488, 490, 496, 497, 499, 500], [4, 7, 8, 9, 11, 13, 15, 16, 17, 20, 22, 26, 37, 39, 43, 48, 52, 57, 63, 64, 65, 72, 75, 78, 80, 86, 87, 89, 91, 92, 94, 95, 100, 102, 104, 106, 114, 123, 124, 126, 127, 132, 136, 137, 138, 146, 156, 162, 167, 172, 181, 182, 183, 190, 191, 199, 201, 204, 208, 209, 212, 216, 222, 231, 237, 239, 241, 245, 252, 263, 264, 266, 268, 274, 282, 283, 289, 290, 292, 293, 296, 297, 300, 305, 311, 315, 323, 327, 335, 338, 339, 344, 351, 357, 358, 359, 362, 367, 370, 374, 376, 377, 379, 380, 383, 386, 398, 399, 409, 411, 412, 419, 420, 421, 425, 428, 438, 441, 446, 447, 452, 455, 456, 458, 460, 465, 469, 472, 473, 474, 478, 480, 481, 486, 494], [5, 19, 29, 30, 31, 36, 41, 42, 44, 51, 71, 76, 81, 90, 93, 110, 119, 120, 128, 131, 135, 145, 159, 165, 166, 173, 185, 186, 195, 214, 224, 225, 230, 235, 242, 244, 248, 251, 257, 260, 267, 269, 271, 273, 281, 285, 286, 291, 295, 301, 306, 308, 309, 319, 320, 324, 325, 326, 331, 334, 336, 346, 349, 369, 372, 378, 393, 394, 396, 402, 431, 442, 448, 457, 466, 467, 470, 479, 482, 483, 487, 489, 492, 493], [10, 12, 23, 24, 33, 34, 38, 50, 53, 56, 58, 60, 62, 73, 74, 77, 83, 85, 103, 115, 116, 118, 121, 122, 139, 140, 141, 142, 149, 157, 158, 160, 161, 168, 174, 175, 177, 180, 187, 189, 194, 196, 197, 203, 215, 223, 228, 229, 233, 243, 246, 253, 259, 262, 265, 278, 279, 298, 307, 328, 332, 342, 350, 363, 364, 373, 385, 387, 390, 392, 395, 403, 404, 413, 422, 427, 444, 461, 463, 471, 475, 484, 491, 495], [150]]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Use the Louvain algorithm to find the communities in the graph\n",
    "partition = community_louvain.best_partition(G, weight = 'weight')\n",
    "print(\"Modularity: \", community_louvain.modularity(partition, G))\n",
    "\n",
    "# Count the number of nodes in each community\n",
    "community_count = Counter(partition.values())\n",
    "\n",
    "# Print the communities found in the graph\n",
    "prediction=[]\n",
    "for com in set(partition.values()) :\n",
    "    list_nodes = [nodes for nodes in partition.keys() if partition[nodes] == com]\n",
    "    prediction.append(list_nodes)\n",
    "    print(\"Community \", com+1, \": \", list_nodes, \", count: \", community_count[com])\n",
    "\n",
    "print(prediction)\n",
    "nx.set_node_attributes(G, partition, 'community')\n",
    "\n",
    "nx.write_gexf(G, 'graph_title.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true:[1, 1, 3, 3, 1, 3, 2, 2, 2, 5, 1, 2, 2, 1, 2, 3, 2, 3, 4, 2, 5, 2, 1, 1, 3, 2, 1, 3, 3, 2, 1, 3, 1, 5, 1, 1, 2, 1, 2, 1, 4, 6, 2, 1, 1, 1, 3, 1, 1, 4, 1, 2, 1, 1, 1, 5, 2, 1, 1, 1, 1, 1, 2, 2, 2, 4, 3, 3, 1, 3, 3, 2, 1, 1, 2, 3, 1, 2, 3, 2, 4, 1, 1, 3, 1, 2, 2, 3, 2, 4, 2, 2, 3, 2, 2, 1, 1, 3, 3, 2, 1, 3, 1, 2, 1, 2, 3, 1, 1, 1, 2, 1, 3, 2, 1, 5, 1, 1, 1, 3, 1, 1, 2, 2, 1, 2, 2, 1, 2, 1, 1, 2, 3, 3, 4, 2, 3, 2, 5, 1, 1, 2, 3, 2, 3, 2, 3, 3, 5, 1, 1, 3, 3, 3, 3, 2, 3, 1, 1, 1, 1, 2, 1, 3, 2, 4, 2, 2, 1, 3, 1, 5, 3, 1, 1, 1, 4, 1, 1, 1, 1, 2, 2, 3, 4, 2, 1, 3, 1, 2, 2, 1, 1, 3, 3, 5, 1, 1, 2, 3, 2, 3, 1, 4, 1, 1, 3, 1, 2, 3, 1, 4, 3, 4, 1, 2, 4, 2, 3, 2, 1, 2, 5, 1, 1, 3, 5, 1, 1, 4, 2, 1, 1, 1, 1, 3, 2, 4, 2, 2, 2, 3, 4, 3, 2, 1, 1, 3, 3, 1, 3, 1, 1, 1, 1, 4, 1, 1, 1, 3, 5, 1, 2, 4, 1, 1, 4, 2, 6, 1, 3, 4, 3, 2, 1, 3, 1, 1, 1, 1, 4, 2, 1, 3, 1, 4, 4, 3, 2, 2, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 3, 1, 3, 2, 4, 1, 4, 3, 3, 2, 2, 1, 1, 2, 1, 1, 1, 1, 4, 3, 1, 2, 1, 4, 3, 2, 1, 1, 4, 3, 1, 3, 5, 1, 1, 1, 2, 2, 1, 1, 1, 3, 2, 4, 3, 1, 3, 1, 1, 3, 3, 3, 3, 1, 3, 2, 2, 1, 1, 1, 2, 1, 1, 3, 2, 2, 3, 1, 2, 3, 4, 5, 2, 3, 2, 3, 3, 2, 2, 1, 1, 1, 1, 3, 2, 1, 3, 3, 1, 3, 1, 1, 1, 3, 1, 1, 3, 2, 3, 1, 4, 1, 2, 1, 3, 1, 4, 2, 1, 3, 2, 1, 3, 2, 3, 3, 1, 2, 2, 2, 1, 3, 3, 2, 4, 4, 2, 1, 1, 1, 1, 3, 1, 3, 1, 1, 2, 1, 3, 2, 2, 3, 1, 1, 2, 2, 1, 3, 1, 1, 2, 1, 4, 2, 2, 1, 2, 3, 2, 5, 3, 1, 1, 2, 1, 1, 1, 3, 1, 1, 2, 2, 2, 1, 1, 2, 2, 3, 2, 2, 4, 3, 1, 1, 2, 1, 3, 2, 3, 1, 1, 6, 2, 2, 1, 3, 1, 3, 1]\n",
      "\n",
      "pred:[1, 1, 2, 3, 4, 2, 3, 3, 3, 5, 3, 5, 3, 2, 3, 3, 3, 2, 4, 3, 1, 3, 5, 5, 2, 3, 1, 2, 4, 4, 4, 2, 5, 5, 1, 4, 3, 5, 3, 1, 4, 4, 3, 4, 1, 1, 2, 3, 1, 5, 4, 3, 5, 1, 2, 5, 3, 5, 1, 5, 1, 5, 3, 3, 3, 1, 2, 2, 1, 2, 4, 3, 5, 5, 3, 4, 5, 3, 2, 3, 4, 1, 5, 2, 5, 3, 3, 2, 3, 4, 3, 3, 4, 3, 3, 1, 1, 2, 2, 3, 1, 3, 5, 3, 2, 3, 2, 1, 1, 4, 2, 1, 2, 3, 5, 5, 1, 5, 4, 4, 5, 5, 3, 3, 1, 3, 3, 4, 2, 2, 4, 3, 2, 2, 4, 3, 3, 3, 5, 5, 5, 5, 2, 2, 4, 3, 2, 2, 5, 6, 1, 2, 2, 2, 2, 3, 5, 5, 4, 5, 5, 3, 2, 2, 4, 4, 3, 5, 1, 2, 2, 3, 4, 5, 5, 2, 5, 1, 1, 5, 3, 3, 3, 2, 4, 4, 5, 2, 5, 3, 3, 1, 1, 5, 4, 5, 5, 1, 3, 2, 3, 2, 5, 3, 1, 1, 2, 3, 3, 2, 1, 3, 2, 4, 5, 3, 1, 2, 2, 1, 1, 3, 5, 4, 4, 1, 1, 5, 5, 4, 3, 1, 5, 1, 4, 2, 3, 1, 3, 1, 3, 4, 5, 4, 3, 5, 1, 4, 2, 1, 4, 3, 5, 1, 1, 1, 4, 1, 5, 4, 1, 5, 3, 3, 5, 3, 4, 3, 4, 1, 4, 1, 4, 3, 1, 2, 1, 5, 5, 1, 4, 3, 3, 2, 4, 4, 1, 2, 3, 3, 4, 3, 3, 1, 4, 3, 3, 5, 1, 3, 4, 2, 1, 2, 3, 4, 5, 4, 4, 2, 3, 2, 1, 1, 3, 2, 1, 1, 4, 4, 2, 1, 3, 4, 4, 4, 3, 5, 1, 1, 4, 5, 2, 4, 3, 4, 1, 3, 3, 2, 2, 5, 2, 3, 1, 4, 1, 2, 4, 5, 3, 2, 2, 2, 1, 2, 3, 3, 3, 1, 1, 3, 5, 5, 2, 1, 3, 2, 4, 3, 2, 4, 5, 3, 2, 3, 3, 4, 3, 3, 1, 2, 3, 1, 5, 3, 5, 2, 2, 5, 2, 5, 4, 4, 5, 4, 1, 3, 3, 2, 1, 4, 5, 5, 1, 2, 1, 1, 3, 1, 3, 3, 5, 2, 2, 2, 2, 1, 3, 3, 3, 5, 2, 2, 3, 1, 5, 3, 2, 1, 4, 1, 2, 1, 2, 1, 1, 3, 1, 2, 3, 4, 2, 5, 1, 3, 3, 4, 2, 2, 1, 3, 1, 2, 3, 3, 4, 3, 2, 3, 5, 2, 5, 1, 3, 4, 4, 1, 3, 4, 5, 3, 3, 3, 5, 1, 1, 3, 4, 3, 3, 4, 4, 5, 1, 3, 4, 2, 4, 2, 5, 4, 4, 3, 5, 2, 2, 1, 2, 2]\n",
      "Fowlkes-Mallows score as percentage: 50.90%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import fowlkes_mallows_score\n",
    "\n",
    "df = df.assign(Category=lambda x: 'None')\n",
    "df['Category'] = df.apply(lambda x: 'Computer Science' if x['Computer Science'] == 1 else 'Physics' if x['Physics'] == 1 else 'Mathematics' if x['Mathematics'] == 1 else 'Statistics' if x['Statistics'] == 1 else 'Quantitative Biology' if x['Quantitative Biology'] == 1 else 'Quantitative Finance' if x['Quantitative Finance'] == 1 else 'None', axis=1)\n",
    "\n",
    "category_counts = df['Category'].value_counts()\n",
    "category_groups = df.groupby('Category')\n",
    "category_lists = [df.loc[df['Category'] == category, 'ID'].tolist() for category in category_counts.index]\n",
    "\n",
    "\n",
    "true_categories = [0 for i in range(500)]\n",
    "for k in range(len(category_lists)):\n",
    "    for i in range(len(category_lists[k])):\n",
    "        true_categories[category_lists[k][i]-1] = k+1\n",
    "\n",
    "\n",
    "predicted_communties = [0 for i in range(500)]\n",
    "for k in range(len(prediction)):\n",
    "    for i in range(len(prediction[k])):\n",
    "        predicted_communties[prediction[k][i]-1] = k+1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"true:{true_categories}\\n\\npredicted:{predicted_communties}\")\n",
    "\n",
    "Community_Score = fowlkes_mallows_score(true_categories,predicted_communties)\n",
    "#print(Community_Score)\n",
    "# Convert score to percentage\n",
    "percentage = Community_Score * 100\n",
    "print(\"Fowlkes-Mallows score as percentage: {:.2f}%\".format(percentage))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fowlkes-Mallows score as percentage: 56.38%\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df = df.head(500)\n",
    "\n",
    "# Load the sentence transformer model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate sentence embeddings for the articles using their Abstracts \n",
    "sentences = df['ABSTRACT'].tolist()\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Create a NetworkX graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes to the graph, one for each article\n",
    "for i, row in df.iterrows():\n",
    "    G.add_node(row['ID'], title=row['TITLE'])\n",
    "\n",
    "# Add edges to the graph, connecting nodes that are similar according the cosine similarity between their embeddings\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(i+1, len(sentences)):\n",
    "        # Calculate the similarity between the ith and jth articles\n",
    "        # using the cosine similarity between their embeddings\n",
    "        sim = cosine_similarity(embeddings[i].reshape(1, -1), embeddings[j].reshape(1, -1))[0][0]\n",
    "        \n",
    "        # If the similarity is above some threshold, add an edge between the nodes\n",
    "        if sim > 0.2:\n",
    "            G.add_edge(df.iloc[i]['ID'], df.iloc[j]['ID'], weight=sim)\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Use the Louvain algorithm to find the communities in the graph\n",
    "partition = community_louvain.best_partition(G, weight = 'weight')\n",
    "# print(\"Modularity: \", community_louvain.modularity(partition, G))\n",
    "\n",
    "# Count the number of nodes in each community\n",
    "community_count = Counter(partition.values())\n",
    "\n",
    "# Print the communities found in the graph\n",
    "prediction=[]\n",
    "for com in set(partition.values()) :\n",
    "    list_nodes = [nodes for nodes in partition.keys() if partition[nodes] == com]\n",
    "    prediction.append(list_nodes)\n",
    "    #print(\"Community \", com+1, \": \", list_nodes, \", count: \", community_count[com])\n",
    "\n",
    "# print(prediction)\n",
    "nx.set_node_attributes(G, partition, 'community')\n",
    "\n",
    "nx.write_gexf(G, 'graph_abstract.gexf')\n",
    "\n",
    "\n",
    "from sklearn.metrics import fowlkes_mallows_score\n",
    "\n",
    "df = df.assign(Category=lambda x: 'None')\n",
    "df['Category'] = df.apply(lambda x: 'Computer Science' if x['Computer Science'] == 1 else 'Physics' if x['Physics'] == 1 else 'Mathematics' if x['Mathematics'] == 1 else 'Statistics' if x['Statistics'] == 1 else 'Quantitative Biology' if x['Quantitative Biology'] == 1 else 'Quantitative Finance' if x['Quantitative Finance'] == 1 else 'None', axis=1)\n",
    "\n",
    "category_counts = df['Category'].value_counts()\n",
    "category_groups = df.groupby('Category')\n",
    "category_lists = [df.loc[df['Category'] == category, 'ID'].tolist() for category in category_counts.index]\n",
    "\n",
    "\n",
    "true_categories = [0 for i in range(500)]\n",
    "for k in range(len(category_lists)):\n",
    "    for i in range(len(category_lists[k])):\n",
    "        true_categories[category_lists[k][i]-1] = k+1\n",
    "\n",
    "\n",
    "predicted_communties = [0 for i in range(500)]\n",
    "for k in range(len(prediction)):\n",
    "    for i in range(len(prediction[k])):\n",
    "        predicted_communties[prediction[k][i]-1] = k+1\n",
    "\n",
    "# print(true_categories)\n",
    "# print(predicted_communties)\n",
    "\n",
    "Community_Score = fowlkes_mallows_score(true_categories,predicted_communties)\n",
    "# Convert score to percentage\n",
    "percentage = Community_Score * 100\n",
    "print(\"Fowlkes-Mallows score as percentage: {:.2f}%\".format(percentage))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
